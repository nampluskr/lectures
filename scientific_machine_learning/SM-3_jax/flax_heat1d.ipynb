{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nnx.Module):\n",
    "    def __init__(self, din: int, dmid: int, dout: int, *, rngs: nnx.Rngs):\n",
    "        self.linear1 = nnx.Linear(din, dmid, rngs=rngs)\n",
    "        self.activation = nnx.tanh\n",
    "        self.linear2 = nnx.Linear(dmid, dout, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        x = jnp.hstack([x, t])\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1) (20, 1) (20, 1)\n"
     ]
    }
   ],
   "source": [
    "model = MLP(2, 32, 1, rngs=nnx.Rngs(0))\n",
    "\n",
    "x = jnp.ones((20, 1))*0.01\n",
    "t = jnp.ones((20, 1))*0.01\n",
    "u = jnp.ones((20, 1))*0.01\n",
    "\n",
    "u_pred = model(x, t)\n",
    "\n",
    "print(x.shape, t.shape, u_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.0113041, dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def residual_loss(model, x, t):\n",
    "    u = model(x, t)\n",
    "    u_t = jax.jacrev(model, argnums=1)(x, t)\n",
    "    # u_x = jax.jacrev(model, argnums=0)(x, t)\n",
    "    u_xx = jax.jacrev(jax.jacrev(model, argnums=0), argnums=0)(x, t)\n",
    "    residual = u_t - u_xx\n",
    "    return jnp.mean(residual**2)\n",
    "\n",
    "residual_loss(model, x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, x, t, u_true):\n",
    "    def loss_fn(model):\n",
    "        u = model(x, t)\n",
    "        u_t = jax.jacrev(model, argnums=1)(x, t)\n",
    "        # u_x = jax.jacrev(model, argnums=0)(x, t)\n",
    "        u_xx = jax.jacrev(jax.jacrev(model, argnums=0), argnums=0)(x, t)\n",
    "        residual = u_t - u_xx\n",
    "        loss_pde = jnp.mean(residual**2)\n",
    "        loss_mse = jnp.mean((u - u_true)**2)\n",
    "        return loss_pde + loss_mse\n",
    "    \n",
    "    grads = nnx.grad(loss_fn)(model)\n",
    "    _, params, rest = nnx.split(model, nnx.Param, ...)\n",
    "    params = jax.tree.map(lambda p, g: p - 0.01 * g, params, grads)\n",
    "    nnx.update(model, nnx.GraphState.merge(params, rest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(model, x, t, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum))\n",
    "metrics = nnx.MultiMetric(\n",
    "  accuracy=nnx.metrics.Accuracy(),\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "\n",
    "def loss_fn(model, x, t, u):\n",
    "    u_pred = model(x, t)\n",
    "    loss = jnp.mean((u_pred - u)**2)\n",
    "    return loss, u_pred\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, x, t, u):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, u_pred), grads = grad_fn(model, x, t, u)\n",
    "    # metrics.update(loss=loss, logits=u_pred, labels=u)  # In-place updates.\n",
    "    optimizer.update(grads)                               # In-place updates.\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model, metrics: nnx.MultiMetric, x, t, u):\n",
    "    loss, u_pred = loss_fn(model, x, t, u)\n",
    "    # metrics.update(loss=loss, logits=u_pred, labels=u)  # In-place updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(model, optimizer, x, t, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  10/100] loss: 7.23e-03\n",
      "[  20/100] loss: 1.44e-03\n",
      "[  30/100] loss: 6.35e-05\n",
      "[  40/100] loss: 1.58e-04\n",
      "[  50/100] loss: 9.11e-05\n",
      "[  60/100] loss: 6.07e-06\n",
      "[  70/100] loss: 7.16e-06\n",
      "[  80/100] loss: 3.90e-06\n",
      "[  90/100] loss: 2.09e-07\n",
      "[ 100/100] loss: 5.08e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array(0.00722719, dtype=float32),\n",
       " Array(0.00143648, dtype=float32),\n",
       " Array(6.3463885e-05, dtype=float32),\n",
       " Array(0.00015777, dtype=float32),\n",
       " Array(9.109734e-05, dtype=float32),\n",
       " Array(6.0693014e-06, dtype=float32),\n",
       " Array(7.158123e-06, dtype=float32),\n",
       " Array(3.896335e-06, dtype=float32),\n",
       " Array(2.0872065e-07, dtype=float32),\n",
       " Array(5.080557e-07, dtype=float32)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(2, 32, 1, rngs=nnx.Rngs(0))\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum))\n",
    "metrics = nnx.MultiMetric(\n",
    "#   accuracy=nnx.metrics.Accuracy(),\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "\n",
    "x = jnp.ones((20, 1))*0.01\n",
    "t = jnp.ones((20, 1))*0.01\n",
    "u = jnp.ones((20, 1))*0.01\n",
    "\n",
    "def loss_fn(model, x, t, u):\n",
    "    pred = model(x, t)\n",
    "    u_t = jax.jacrev(model, argnums=1)(x, t)\n",
    "    u_xx = jax.jacrev(jax.jacrev(model, argnums=0), argnums=0)(x, t)\n",
    "\n",
    "    residual = u_t - u_xx\n",
    "    loss_pde = jnp.mean(residual**2)\n",
    "    loss_mse = jnp.mean((pred - u)**2)\n",
    "    loss = loss_pde + loss_mse\n",
    "    return loss, pred\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, metrics, x, t, u):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, pred), grads = grad_fn(model, x, t, u)\n",
    "    # metrics.update(loss=loss, logits=u_pred, labels=u)  # In-place updates.\n",
    "    metrics.update(loss=loss)  # In-place updates.\n",
    "    optimizer.update(grads)                             # In-place updates.\n",
    "\n",
    "\n",
    "metrics_history = {\"train_loss\": []}\n",
    "n_epochs = 100\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_step(model, optimizer, metrics, x, t, u)\n",
    "\n",
    "    if epoch % (n_epochs // 10) == 0:\n",
    "        for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "            print(f\"[{epoch:4d}/{n_epochs}] {metric}: {value:.2e}\")\n",
    "            metrics_history[f'train_{metric}'].append(value)\n",
    "        metrics.reset()\n",
    "        \n",
    "metrics_history[\"train_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  10/100] loss: 7.23e-03\n",
      "[  20/100] loss: 4.33e-03\n",
      "[  30/100] loss: 2.91e-03\n",
      "[  40/100] loss: 2.22e-03\n",
      "[  50/100] loss: 1.80e-03\n",
      "[  60/100] loss: 1.50e-03\n",
      "[  70/100] loss: 1.28e-03\n",
      "[  80/100] loss: 1.12e-03\n",
      "[  90/100] loss: 9.99e-04\n",
      "[ 100/100] loss: 8.99e-04\n"
     ]
    }
   ],
   "source": [
    "class TrainState(nnx.Optimizer):\n",
    "    def __init__(self, model, optimizer, metrics):\n",
    "        self.metrics = metrics\n",
    "        super().__init__(model, optimizer)\n",
    "\n",
    "    def update(self, *, grads, **updates):\n",
    "        self.metrics.update(**updates)\n",
    "        super().update(grads)\n",
    "\n",
    "model = MLP(2, 32, 1, rngs=nnx.Rngs(0))\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "metrics = nnx.metrics.Average()\n",
    "state = TrainState(model, optax.adamw(learning_rate, momentum), metrics)\n",
    "\n",
    "x = jnp.ones((20, 1))*0.01\n",
    "t = jnp.ones((20, 1))*0.01\n",
    "u = jnp.ones((20, 1))*0.01\n",
    "\n",
    "def loss_fn(model, x, t, u):\n",
    "    pred = model(x, t)\n",
    "    u_t = jax.jacrev(model, argnums=1)(x, t)\n",
    "    u_xx = jax.jacrev(jax.jacrev(model, argnums=0), argnums=0)(x, t)\n",
    "\n",
    "    residual = u_t - u_xx\n",
    "    loss_pde = jnp.mean(residual**2)\n",
    "    loss_mse = jnp.mean((pred - u)**2)\n",
    "    loss = loss_pde + loss_mse\n",
    "    return loss\n",
    "\n",
    "n_epochs = 100\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    grads = nnx.grad(loss_fn)(state.model, x, t, u)\n",
    "    state.update(grads=grads, values=loss_fn(state.model, x, t, u))\n",
    "\n",
    "    if epoch % (n_epochs // 10) == 0:\n",
    "        print(f\"[{epoch:4d}/{n_epochs}] loss: {state.metrics.compute():.2e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
